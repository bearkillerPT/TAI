*****************************************************************
*								*
*		   LTI LangID Corpus, Release 4			*
*			  12 June 2020				*
*								*
*****************************************************************

============
Introduction
============

Welcome to the language identification training corpus from Carnegie Mellon
University's Language Technologies Institute.  Its purpose is to provide a
common base for running experiments on over a thousand languages, allowing
like-for-like comparisons between different approaches by different
researchers.  After installation, the data will have been split into training
and test data (plus devtest/tuning data for the larger languages), with
optional reduced-size training sets and test sets generated at different
string lengths.

The internal name of this corpus, as reflected in the top-level directory
name, is MIL-TALE or "Thousand Languages" (Spanish/Catalan/Esperanto "mil" =
"thousand" and Afrikaans "tale" = "languages").

===========
WHAT'S NEW?
===========

Changes since Release 3:

* More than 180 new core languages (some had been Additional or Sample
  languages) and many new Additional and Sample languages have been added.
  Over 1500 languages are now represented in this corpus.

* All files which used 2-letter ISO 639-1 language codes have been renamed to
  use 3-letter ISO 639-3 codes.  The file naming convention has been modified
  to make it easier to parse out the various attributes of the file.

* Enhanced utility programs.

* Rewrote code/score.C to eliminate dependence on no-longer-maintained
  FramepaC library

* Fixed install scripts -merge option to only combine files with both the
  same language and writing system


===================
Language Categories
===================

The language files in this corpus are split into three categories:

1. Core

Languages for which sufficient text is available to generate quality models.
There is no fixed minimum amount to be included in this category, but the
threshold is typically 300KB.  Generally, Bibles require less text than
Wikipedia and languages with few lexically-similar languages require less
than those with much lexical overlap.  Experience has shown that performance
improves rapidly up to about 300KB of _diverse_, _representative_ text, then
quickly begins slowing, with little additional improvement beyond 1000KB.
Less-diverse training texts show a more gradual increase in performance than
highly-diverse texts.

2. Additional

Languages for which the amount of text is smaller than for the Core category,
but may still be adequate, depending on the application.  Some of the
Wikipedia files in this category have not been manually cleaned.

3. Sample

Languages for which insufficient text is available for building models (some
consist of a single line of text).  They may be useful as test data given
another source of training text or for measuring lexical similarity with the
languages in the Core and Additional categories.  The majority of Wikipedia
files in this category dating from 2014 have not been manually cleaned.


=============
Prerequisites
=============

To use the installation script and other utility programs and scripts located
in the code/ subdirectory, you will need to have available
    bash
    xargs
    xz
    gcc / g++ toolchain
"bash", "xargs", and "xz" are normally installed by default on recent Linux
distributions, and the compiler is often installed by default as well.
Please install from your distribution's repositories as necessary.

The installed corpus will require about 3.5 GB of disk space if not compressed.


=====================
Installing the Corpus
=====================

Change current directory to the directory into which you extracted the
distribution archive (i.e. the directory in which this file resides).  Select
a directory to serve as the top-level directory for the installed corpus, and
issue the command

   code/install.sh

The script will ask you for the location in which to install and several
other options.

The options may also be specified on the command line.  Using the command

   code/install.sh {INSTALLDIR}

(replacing {INSTALLDIR} as appropriate) will perform the installation with
default settings.  If you wish to create single training files for languages
which have both Wikipedia and Bible text, add the -merge flag (to also create
single test/devtest files, use -mergetest).

Similarly, if you wish to install the Additional languages as well as the
Core languages, add the -all flag.  To copy the Sample languages to the
installation directory, add -sample.

The training sets may optionally be compressed with 'xz' to save disk space
(about 2 GB) by adding the -c option.

Additional reduced-size training sets may be generated by adding the -r or
-reduce option followed by a size in bytes.  This option may be given
multiple times to generate multiple reduced-size training sets.  Any training
files larger than the specified size will be subsampled down to approximately
that size (sampling is based on lines of text, so the result may vary from
the target size by a few hundred bytes).

The install script can take advantage of multiple CPUs.  By default, it will
attempt to use as many threads as sysfs reports CPUs, falling back to two
threads if detection fails.  Add "-j N" to use N threads instead.

A fully-loaded installation command would then look like

   code/install.sh -merge -all -sample -c -j 4 -r 500000 {INSTALLDIR}

The installation creates several subdirectories under {INSTALLDIR}:

   bin/			compiled utility programs and scripts
   train/		Training data files
   train-$SIZE/		reduced-size training data files
   test/		Test data files
   devtest/		Tuning/development test files
   devtrain/		Optional training on tuning languages only
   devtrain-$SIZE/	Optional reduced-size training for devtest
   sample/		Language data too small for modeling
   testsets-*/		Evaluation data with various line lengths

Note that only a minority of languages have enough text to generate devtest
files for use in parameter tuning, and some of the smaller languages may
generate insufficient evaluation strings at the longer line lengths.

The installation process takes several minutes.


============
Data Sources
============

Europarl Corpus, version 7 (21 languages)
  European Union parliamentary proceedings.  Retrieved from
  http://www.statmt.org/europarl/.

Wikipedia (278+51 languages, samples for 341 additional)
  Texts in this portion of the corpus were retrieved in May and July 2014 and
  March 2020.  They are licensed under Creative
  Commons-Attribution-ShareAlike 3.0.

bible.com (formerly YouVersion.com) (8+1 languages, samples for 1 additional)
  Bible translations under Creative Commons license, retrieved at various
  times from 2012 through 2015.
  Base URL is http://bible.com/versions/ .

bibles.org (4 languages)
  Bible translations under Creative Commons license, retrieved at various
  times.

door43.org (13+6 languages, samples for 1 additional)
  Bible translations and bible stories under Creative Commons license,
  collected in March and May 2020.
  Base URL is https://git.door43.org/Door43-Catalog .

ebible.org (777+17 languages, samples for 7 additional)
  Bible translations mostly under Creative Commons license; some public
  domain and several under informal non-commercial/ no-derivatives/
  electronic-only license.  Retrieved in June 2017 and (mostly) March 2020.
  Base URL is https://ebible.org/find/ .
  Bible archives are downloaded from https://ebible.org/Scriptures/${ID}_usfm.zip
  (ID is often but not always the ISO639-3 code)

eWord (2 languages)
  Public-domain Bible translations from the eWord project.
  
PNG.Bible (formerly PNGScriptures.org) (5 languages)
  Bible translations under Creative Commons license, retrieved at various
  times.
  https://png.bible/${ISO639}/${ISO639}_usfm.zip

Public Domain (1 language)
  Bible translations which are sufficiently old to have fallen into the
  public domain.

ScriptureEarth.org (113+3 languages, samples for 35 additional)
  Bible translations under Creative Commons license, retrieved at various
  times.  Most files are copyrighted by Wycliffe Bible Translators, Inc.
  https://www.scriptureearth.org/data/${ISO639}/

Project Gutenberg (1 language)
  Public-domain texts.

A few languages have both Wikipedia and Bible data, or are in
different categories for different scripts.  Overall, the corpus
contains 1151 core languages, 77 additional languages, and 381 sample
languages, for a grand total of 1546.


================
Data preparation
================

Bibles from ScriptureEarth come in three varieties:
1) Older Bibles were distributed as self-extracting ZIP archives, with an
   installer integrated in the self-extractor.  For these, the original text
   file in the archive was extracted and re-compressed with "xz".  The script
   to split data into training and test sets will also strip out the markup.

2) For most newer Bibles, the per-book "SFM" files have been downloaded from
   scriptureearth.com/data/$LANGCODE/viewer and packaged in an xz-compressed
   tarball.  The script to split data into training and test sets will also
   strip out the markup.

3) Where no "SFM" files are available, the *.mybible file has been downloaded
   from scriptureearth.com/data/$LANGCODE/study, the verses extracted from
   the SQLite3 file, and the resulting text compressed with "xz".

For YouVersion(bible.com) and bibles.org, the web pages for individual
chapters were downloaded and the HTML markup removed.  The resulting
plain-text files were packaged into xz-compressed tarballs.

For PNG.Bible and ebible.org, the original ZIP archives are included here,
converted to xz-compressed tarballs, along with code to extract the actual
Bible verses where necessary.

For Europarl, the files for May and June 2011 were used for each language
(Slovenian also added April 2011 to bring it closer to 4 million bytes).  The
SGML metadata was stripped, some conservative sentence splitting applied, and
the result de-duplicated using
   "sort -u".
Language identification was applied to eliminate a small number of lines
containing text in other languages (such as titles), and the remaining text
down-sampled to approximately 4 million bytes per language (7.5 million for
Bulgarian and Greek, which are predominantly two bytes per character).

For Wikipedia, a list of all pages in the language's Wiki was retrieved from
   $LANGCODE.wikipedia.org/wiki/Special:AllPages
limited to a maximum of 15,000 pages via uniform sampling.  Those pages were
then retrieved, the content extracted from the HTML (May 2014) or XML (July
2014/May 2020), some automatic filtering applied, the text deduplicated with
"sort -u" and a custom fuzzy variant of "uniq", and then further manually
cleaned (including deduplication of templated text such as "$CITY is a city
in $COUNTRY, with a population of $POP" to avoid having large numbers of such
entries skew the n-gram statistics).  The cleaned text was then downsampled
to the target size of 4 or 7.5 megabytes if necessary.


======================
File-naming Convention
======================

The data files have a common naming convention to make it clear which
language and script are contained in the file, the source of the data, and
(for output files) the purpose of the file.

All files start with the ISO 639 alphabetic code for the language, optionally
followed by a hyphen and a code or name for the dialect within the language
code.  This is followed by an underscore and a country code or XX for extinct
or constructed languages; the country code may be used to distinguish
regional variants of a language, e.g. British versus American English.

The country code is followed by a period and the four-letter ISO 15924
script code, e.g. "latn" (Latin-based alphabet), "cyrl" (Cyrillic),
"grek" (Greek), "ethi" (Ethiopic), or "hant" (traditional Chinese
characters).

The script code is followed by a period and the human-readable
language name.  After a futher period, one or more additional fields
follow, separated by hyphens:
  data source ("bible", "EP7", "PG", "wiki", etc.)
  "PD" for public-domain files
  purpose ("train", "test", "devtest")
  orthography ("tradortho", "newortho", "altortho")

Finally, the file type is indicated by extensions .txz (xz-compressed
tar file), .utf8.xz (xz-compressed plain text), .copr.txt
(copyright license statement), etc.

The extension "utf8" simply indicates a text file in the UTF-8 encoding.
This is largely historic (there were also .ascii, .latin1, etc. to
distinguish different encodings), but has been kept because it also serves to
distinguish the corpus files from other text files, such as the processed
test-set files, which use the standard .txt extension.

Thus, a full sample file name might be
  eng_US.latn.English.PD-bible.utf8.xz
for a public-domain American English bible using the Latin alphabet.


==============
Redistribution
==============

With the exception of the contents of the Europarl/, ProjectGutenberg/, and
PublicDomain/ directories, all code and text in this corpus are copyrighted.
However, they may be redistributed under the terms of various Creative
Commons licenses and the GNU GPL.  Copying the unmodified archive
noncommercially is permitted by all of the licenses.  For commercial
redistribution or redistribution of modified versions, please consult the
individual licenses.


================
Utility Programs
================

Once unpacked, the code/ subdirectory contains a number of utility programs
in addition to the installation script and the programs it invokes.

countlangs.sh
-------------

Count the number of different languages in the corpus and in the largest of
its directories.

Usage: countlangs.sh [basedir]

If not specified, "basedir" defaults to the ../text directory relative to the
location of the script (i.e. if run from the corpus's /code directory, it
will find the files).


dedup
-----

Remove duplicate and near-duplicate lines from a file.

Usage: dedup [options] <in >out

TODO: describe options


icuconv
-------

Convert files between UTF8 and any other character encoding supported by libicu.

Usage: icuconv {f|t} encoding [file ...] >out

The first argument specifies the direction of conversion: (f)rom 'encoding'
or (t)o 'encoding'.  Each file is read in turn, converted, and the converted
text written to standard output.  If no file name is given on the command
line, standard input is read.


icutrans
--------

Use libicu to transliterate text.

Usage: icutrans trans [file ...] >out

Apply transliteration 'trans' to the named files (or standard input if no
names are given) and write the result to standard output.  If 'trans' is '=',
list the available transliterations instead.  Some example transliterations
which may be available (depending on the installed version of libicu) are
Cyrillic-Latin, Beng-Deva, Hangul-Latin, and Latin-Hebrew.


interleave
----------

Perform line-wise interleaving of two or more files.  Resulting blocks of
lines can be separated by an inserted blank line, and groups of multiple
lines can be interleaved.

Usage: interleave [options] file1 file2 ... >out

TODO: describe options


subsample
---------

Select a portion of the file given on standard output according to the
criteria given on the command line, and output that selected portion to
standard output.

Usage: subsample [options] count <in >out

By default, subsample selects 'count' lines at random.  With -u, it selects
'count' uniformly spaced lines, while with -i, the count becomes an interval
and subsample selects every 'count'th line from the input.  Finally, with -b,
the count becomes the number of bytes to select instead of the number of
lines.  A line is selected whenever the total output in bytes up to that
point falls below the desired proportion of the input.

In addition to the above options, lines can also be filtered based on content
before the above options are applied.  Three filters are available: minimum
length in bytes (-l${MIN}), maximum length in bytes (-L$(MAX)), and majority
of characters in a given writing system (-c${SCRIPT}), where $SCRIPT is a
four-letter ISO 15924 script identifier (e.g. "latn" for Latin, "cyrl" for
Cyrillic, "grek" for Greek).  The minimum percentage of characters required
to be in the given script can be controlled with -C${PERCENT}, where the
allowable percentage ranges from 1 to 99 and defaults to 60.  Note that
filtering by script requires the input to be in UTF8 encoding.

Finally, lines which were NOT selected for output can be written to a
'reject' file specified with the -r${REJFILE} flag.  This allows the input to
be split into a pair of files which together contain the entire input.  Note
that the order of lines in the reject file will not match the input if both
count-based and content-based filtering are used; lines rejected by the
content-based filtering will occur first.


utf8hist
--------

Display statistics about the codepoints present in a UTF8-encoded file.  By
default, it displays the counts in each 128-codepoint range, i.e (U+0000 to
U+007F, U+0080 to U+00FF, etc.).

Usage: utf8hist [-s] [-S] [file ...]

With -s, utf8hist adds a four-letter ISO 15924 script identifier for each
range.  Note that not all scripts have been coded, and that where a range
contains multiple scripts, only one is shown.

With -S, utf8hist accumulates counts per ISO 15924 script rather than per
128-codepoint block.

If no file is named on the command line, utf8hist reads standard input.



============
Contact Info
============

This corpus is currently maintained by Ralf Brown <ralf@cs.cmu.edu>.


=========
